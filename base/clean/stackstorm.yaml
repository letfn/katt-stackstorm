---
# Source: stackstorm-ha/charts/mongodb/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: stackstorm-mongodb
  namespace: stackstorm
  labels:
    app.kubernetes.io/name: mongodb
    helm.sh/chart: mongodb-10.0.1
    app.kubernetes.io/instance: stackstorm
    app.kubernetes.io/managed-by: Helm
secrets:
  - name: stackstorm-mongodb
---
# Source: stackstorm-ha/charts/rabbitmq/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: stackstorm-rabbitmq
  namespace: "stackstorm"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-8.0.2
    app.kubernetes.io/instance: stackstorm
    app.kubernetes.io/managed-by: Helm
secrets:
  - name: stackstorm-rabbitmq
---
# Source: stackstorm-ha/templates/service-account.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: stackstorm-ha
  labels:
    chart: "stackstorm-ha-0.60.0"
    app: "stackstorm-ha"
    heritage: "Helm"
    release: "stackstorm"
---
# Source: stackstorm-ha/charts/mongodb/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: stackstorm-mongodb
  namespace: stackstorm
  labels:
    app.kubernetes.io/name: mongodb
    helm.sh/chart: mongodb-10.0.1
    app.kubernetes.io/instance: stackstorm
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: mongodb
type: Opaque
data:
  mongodb-root-password:  "OGZBemRua3NkelBGVVdtNGE2OEVmWTduTWhCUGFh"
  mongodb-password:  "WGVMNVJ4d2o3RjBXdDQzdEZaVlRON0g4U2c1WERIbUs="
  mongodb-replica-set-key:  "ODJQSXREcHFyb3RpNVJuZ09BN1VxYkhIN2M2YkZVd3k="
---
# Source: stackstorm-ha/charts/rabbitmq/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: stackstorm-rabbitmq
  namespace: "stackstorm"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-8.0.2
    app.kubernetes.io/instance: stackstorm
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  rabbitmq-password: "OWpTK3cxdTA3TmJIdFprZTFtK2pXNENq"
  rabbitmq-erlang-cookie: "OE1ycVFkQ1E2QVE4VTNNYWNTdWJIRTVScWtTZnZOYVJIenZ4dUZjRw=="
---
# Source: stackstorm-ha/templates/secrets_rabbitmq.yaml
apiVersion: v1
kind: Secret
metadata:
  name: stackstorm-rabbitmq-definitions
  annotations:
    description: A rabbitmq definition which will be loaded by the rabbitmq subchart to enable mirroring for Rabbit HA
  labels:
    app: st2
    tier: backend
    vendor: stackstorm
    chart: "stackstorm-ha-0.60.0"
    release: "stackstorm"
    heritage: "Helm"
type: Opaque
data:
  rabbitmq-definitions.json:  ewogICJ1c2VycyI6IFsKICAgIHsKICAgICAgIm5hbWUiOiAiYWRtaW4iLAogICAgICAicGFzc3dvcmQiOiAiOWpTK3cxdTA3TmJIdFprZTFtK2pXNENqIiwKICAgICAgInRhZ3MiOiAiYWRtaW5pc3RyYXRvciIKICAgIH0KICBdLAogICJwZXJtaXNzaW9ucyI6IFsKICAgIHsKICAgICAgInVzZXIiOiAiYWRtaW4iLAogICAgICAidmhvc3QiOiAiLyIsCiAgICAgICJjb25maWd1cmUiOiAiLioiLAogICAgICAid3JpdGUiOiAiLioiLAogICAgICAicmVhZCI6ICIuKiIKICAgIH0KICBdLAogICJ2aG9zdHMiOiBbCiAgICB7CiAgICAgICJuYW1lIjogIi8iCiAgICB9CiAgXSwKICAicG9saWNpZXMiOiBbCiAgICB7CiAgICAgICJ2aG9zdCI6Ii8iLAogICAgICAibmFtZSI6ImhhIiwKICAgICAgInBhdHRlcm4iOiIiLAogICAgICAiZGVmaW5pdGlvbiI6IHsKICAgICAgICAiaGEtbW9kZSI6ImFsbCIsCiAgICAgICAgImhhLXN5bmMtbW9kZSI6ICJhdXRvbWF0aWMiLAogICAgICAgICJoYS1zeW5jLWJhdGNoLXNpemUiOjEwCiAgICAgIH0KICAgIH0KICBdCn0K
---
# Source: stackstorm-ha/templates/secrets_ssh.yaml
apiVersion: v1
kind: Secret
metadata:
  name: stackstorm-st2-ssh
  annotations:
    description: StackStorm SSH secret key for 'stanley' user, used to run actions on remote machines
  labels:
    app: st2
    tier: backend
    vendor: stackstorm
    chart: "stackstorm-ha-0.60.0"
    release: "stackstorm"
    heritage: "Helm"
type: Opaque
data:
  # SSH private key for the 'stanley' system user ('system_user.ssh_key_file' in st2.conf).
  private_key: "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"
---
# Source: stackstorm-ha/templates/secrets_st2apikeys.yaml
apiVersion: v1
kind: Secret
metadata:
  name: stackstorm-st2-apikeys
  annotations:
    description: A list of StackStorm API keys with metadata that will be imported into the system
  labels:
    app: st2
    tier: backend
    vendor: stackstorm
    chart: "stackstorm-ha-0.60.0"
    release: "stackstorm"
    heritage: "Helm"
type: Opaque
data:
  apikeys.yaml: "bnVsbA=="
---
# Source: stackstorm-ha/templates/secrets_st2auth.yaml
apiVersion: v1
kind: Secret
metadata:
  name: stackstorm-st2-auth
  annotations:
    description: StackStorm username and password, used for basic .htaccess auth
  labels:
    app: st2
    tier: backend
    vendor: stackstorm
    chart: "stackstorm-ha-0.60.0"
    release: "stackstorm"
    heritage: "Helm"
type: Opaque
data:
  # Username, used to login to StackStorm system (default: st2admin)
  username: "c3QyYWRtaW4="
  # Password, used to login to StackStorm system (default: Ch@ngeMe)
  password: "Q2hAbmdlTWU="
---
# Source: stackstorm-ha/templates/secrets_st2kv.yaml
apiVersion: v1
kind: Secret
metadata:
  name: stackstorm-st2-kv
  annotations:
    description: Key/Value pairs to save in StackStorm's datastore
  labels:
    app: st2
    tier: backend
    vendor: stackstorm
    chart: "stackstorm-ha-0.60.0"
    release: "stackstorm"
    heritage: "Helm"
type: Opaque
data:
  st2kv.yaml: "bnVsbA=="
---
# Source: stackstorm-ha/charts/mongodb/templates/replicaset/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: stackstorm-mongodb-scripts
  namespace: stackstorm
  labels:
    app.kubernetes.io/name: mongodb
    helm.sh/chart: mongodb-10.0.1
    app.kubernetes.io/instance: stackstorm
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: mongodb
data:
  setup.sh: |-
    #!/bin/bash

    echo "Advertised Hostname: $MONGODB_ADVERTISED_HOSTNAME"

    if [[ "$MY_POD_NAME" = "stackstorm-mongodb-0" ]]; then
        echo "Pod name matches initial primary pod name, configuring node as a primary"
        export MONGODB_REPLICA_SET_MODE="primary"
    else
        echo "Pod name doesn't match initial primary pod name, configuring node as a secondary"
        export MONGODB_REPLICA_SET_MODE="secondary"
        export MONGODB_INITIAL_PRIMARY_ROOT_PASSWORD="$MONGODB_ROOT_PASSWORD"
        export MONGODB_INITIAL_PRIMARY_PORT_NUMBER="$MONGODB_PORT_NUMBER"
        export MONGODB_ROOT_PASSWORD="" MONGODB_USERNAME="" MONGODB_DATABASE="" MONGODB_PASSWORD=""
    fi

    exec /opt/bitnami/scripts/mongodb/entrypoint.sh /opt/bitnami/scripts/mongodb/run.sh
---
# Source: stackstorm-ha/charts/rabbitmq/templates/configuration.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: stackstorm-rabbitmq-config
  namespace: "stackstorm"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-8.0.2
    app.kubernetes.io/instance: stackstorm
    app.kubernetes.io/managed-by: Helm
data:
  rabbitmq.conf: |-
    ## Username and password
    default_user = admin
    default_pass = CHANGEME
    ## Clustering
    cluster_formation.peer_discovery_backend  = rabbit_peer_discovery_k8s
    cluster_formation.k8s.host = kubernetes.default.svc.cluster.local
    cluster_formation.node_cleanup.interval = 10
    cluster_formation.node_cleanup.only_log_warning = true
    cluster_partition_handling = autoheal
    # queue master locator
    queue_master_locator = min-masters
    # enable guest user
    loopback_users.guest = false
    load_definitions = /app/rabbitmq-definitions.json
---
# Source: stackstorm-ha/charts/redis/templates/configmap-scripts.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: stackstorm-redis-scripts
  namespace: "stackstorm"
  labels:
    app: redis
    chart: redis-12.3.2
    heritage: Helm
    release: stackstorm
data:
  start-node.sh: |
    #!/bin/bash
    is_boolean_yes() {
        local -r bool="${1:-}"
        # comparison is performed without regard to the case of alphabetic characters
        shopt -s nocasematch
        if [[ "$bool" = 1 || "$bool" =~ ^(yes|true)$ ]]; then
            true
        else
            false
        fi
    }

    HEADLESS_SERVICE="stackstorm-redis-headless.stackstorm.svc.cluster.local"
    REDIS_SERVICE="stackstorm-redis.stackstorm.svc.cluster.local"

    export REDIS_REPLICATION_MODE="slave"
    if [[ -z "$(getent ahosts "$HEADLESS_SERVICE" | grep -v "^$(hostname -i) ")" ]]; then
      export REDIS_REPLICATION_MODE="master"
    fi

    if [[ -n $REDIS_PASSWORD_FILE ]]; then
      password_aux=`cat ${REDIS_PASSWORD_FILE}`
      export REDIS_PASSWORD=$password_aux
    fi

    if [[ -n $REDIS_MASTER_PASSWORD_FILE ]]; then
      password_aux=`cat ${REDIS_MASTER_PASSWORD_FILE}`
      export REDIS_MASTER_PASSWORD=$password_aux
    fi

    if [[ "$REDIS_REPLICATION_MODE" == "master" ]]; then
      echo "I am master"
      if [[ ! -f /opt/bitnami/redis/etc/master.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf
      fi
    else
      if [[ ! -f /opt/bitnami/redis/etc/replica.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/replica.conf /opt/bitnami/redis/etc/replica.conf
      fi

      if is_boolean_yes "$REDIS_TLS_ENABLED"; then
        sentinel_info_command="redis-cli -h $REDIS_SERVICE -p 26379 --tls --cert ${REDIS_TLS_CERT_FILE} --key ${REDIS_TLS_KEY_FILE} --cacert ${REDIS_TLS_CA_FILE} sentinel get-master-addr-by-name mymaster"
      else
        sentinel_info_command="redis-cli -h $REDIS_SERVICE -p 26379 sentinel get-master-addr-by-name mymaster"
      fi
      REDIS_SENTINEL_INFO=($($sentinel_info_command))
      REDIS_MASTER_HOST=${REDIS_SENTINEL_INFO[0]}
      REDIS_MASTER_PORT_NUMBER=${REDIS_SENTINEL_INFO[1]}


      # Immediately attempt to connect to the reported master. If it doesn't exist the connection attempt will either hang
      # or fail with "port unreachable" and give no data. The liveness check will then timeout waiting for the redis
      # container to be ready and restart the it. By then the new master will likely have been elected
      if is_boolean_yes "$REDIS_TLS_ENABLED"; then
        sentinel_info_command="redis-cli -h $REDIS_MASTER_HOST -p 26379 --tls --cert ${REDIS_TLS_CERT_FILE} --key ${REDIS_TLS_KEY_FILE} --cacert ${REDIS_TLS_CA_FILE} sentinel get-master-addr-by-name mymaster"
      else
        sentinel_info_command="redis-cli -h $REDIS_MASTER_HOST -p 26379 sentinel get-master-addr-by-name mymaster"
      fi

      if [[ ! ($($sentinel_info_command)) ]]; then
        # master doesn't actually exist, this probably means the remaining pods haven't elected a new one yet
        # and are reporting the old one still. Once this happens the container will get stuck and never see the new
        # master. We stop here to allow the container to not pass the liveness check and be restarted.
        exit 1
      fi
    fi

    if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then
      cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
    fi
    ARGS=("--port" "${REDIS_PORT}")

    if [[ "$REDIS_REPLICATION_MODE" == "slave" ]]; then
      ARGS+=("--slaveof" "${REDIS_MASTER_HOST}" "${REDIS_MASTER_PORT_NUMBER}")
    fi
    ARGS+=("--protected-mode" "no")

    if [[ "$REDIS_REPLICATION_MODE" == "master" ]]; then
      ARGS+=("--include" "/opt/bitnami/redis/etc/master.conf")
    else
      ARGS+=("--include" "/opt/bitnami/redis/etc/replica.conf")
    fi

    ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
    exec /run.sh "${ARGS[@]}"

  start-sentinel.sh: |
    #!/bin/bash
    replace_in_file() {
        local filename="${1:?filename is required}"
        local match_regex="${2:?match regex is required}"
        local substitute_regex="${3:?substitute regex is required}"
        local posix_regex=${4:-true}

        local result

        # We should avoid using 'sed in-place' substitutions
        # 1) They are not compatible with files mounted from ConfigMap(s)
        # 2) We found incompatibility issues with Debian10 and "in-place" substitutions
        del=$'\001' # Use a non-printable character as a 'sed' delimiter to avoid issues
        if [[ $posix_regex = true ]]; then
            result="$(sed -E "s${del}${match_regex}${del}${substitute_regex}${del}g" "$filename")"
        else
            result="$(sed "s${del}${match_regex}${del}${substitute_regex}${del}g" "$filename")"
        fi
        echo "$result" > "$filename"
    }
    sentinel_conf_set() {
        local -r key="${1:?missing key}"
        local value="${2:-}"

        # Sanitize inputs
        value="${value//\\/\\\\}"
        value="${value//&/\\&}"
        value="${value//\?/\\?}"
        [[ "$value" = "" ]] && value="\"$value\""

        replace_in_file "/opt/bitnami/redis-sentinel/etc/sentinel.conf" "^#*\s*${key} .*" "${key} ${value}" false
    }
    sentinel_conf_add() {
        echo $'\n'"$@" >> "/opt/bitnami/redis-sentinel/etc/sentinel.conf"
    }
    is_boolean_yes() {
        local -r bool="${1:-}"
        # comparison is performed without regard to the case of alphabetic characters
        shopt -s nocasematch
        if [[ "$bool" = 1 || "$bool" =~ ^(yes|true)$ ]]; then
            true
        else
            false
        fi
    }
    host_id() {
      echo "$1" | openssl sha1 | awk '{print $2}'
    }

    HEADLESS_SERVICE="stackstorm-redis-headless.stackstorm.svc.cluster.local"
    REDIS_SERVICE="stackstorm-redis.stackstorm.svc.cluster.local"

    if [[ -n $REDIS_PASSWORD_FILE ]]; then
      password_aux=`cat ${REDIS_PASSWORD_FILE}`
      export REDIS_PASSWORD=$password_aux
    fi

    if [[ ! -f /opt/bitnami/redis-sentinel/etc/sentinel.conf ]]; then
      cp /opt/bitnami/redis-sentinel/mounted-etc/sentinel.conf /opt/bitnami/redis-sentinel/etc/sentinel.conf
    fi

    export REDIS_REPLICATION_MODE="slave"
    if [[ -z "$(getent ahosts "$HEADLESS_SERVICE" | grep -v "^$(hostname -i) ")" ]]; then
      export REDIS_REPLICATION_MODE="master"
    fi

    if [[ "$REDIS_REPLICATION_MODE" == "master" ]]; then
      REDIS_MASTER_HOST="$(hostname -i)"
      REDIS_MASTER_PORT_NUMBER="6379"
    else
      if is_boolean_yes "$REDIS_SENTINEL_TLS_ENABLED"; then
        sentinel_info_command="redis-cli -h $REDIS_SERVICE -p 26379 --tls --cert ${REDIS_SENTINEL_TLS_CERT_FILE} --key ${REDIS_SENTINEL_TLS_KEY_FILE} --cacert ${REDIS_SENTINEL_TLS_CA_FILE} sentinel get-master-addr-by-name mymaster"
      else
        sentinel_info_command="redis-cli -h $REDIS_SERVICE -p 26379 sentinel get-master-addr-by-name mymaster"
      fi
      REDIS_SENTINEL_INFO=($($sentinel_info_command))
      REDIS_MASTER_HOST=${REDIS_SENTINEL_INFO[0]}
      REDIS_MASTER_PORT_NUMBER=${REDIS_SENTINEL_INFO[1]}

      # Immediately attempt to connect to the reported master. If it doesn't exist the connection attempt will either hang
      # or fail with "port unreachable" and give no data. The liveness check will then timeout waiting for the sentinel
      # container to be ready and restart the it. By then the new master will likely have been elected
      if is_boolean_yes "$REDIS_SENTINEL_TLS_ENABLED"; then
        sentinel_info_command="redis-cli -h $REDIS_MASTER_HOST -p 26379 --tls --cert ${REDIS_SENTINEL_TLS_CERT_FILE} --key ${REDIS_SENTINEL_TLS_KEY_FILE} --cacert ${REDIS_SENTINEL_TLS_CA_FILE} sentinel get-master-addr-by-name mymaster"
      else
        sentinel_info_command="redis-cli -h $REDIS_MASTER_HOST -p 26379 sentinel get-master-addr-by-name mymaster"
      fi

      if [[ ! ($($sentinel_info_command)) ]]; then
        # master doesn't actually exist, this probably means the remaining pods haven't elected a new one yet
        # and are reporting the old one still. Once this happens the container will get stuck and never see the new
        # master. We stop here to allow the container to not pass the liveness check and be restarted.
        exit 1
      fi
    fi
    sentinel_conf_set "sentinel monitor" "mymaster "$REDIS_MASTER_HOST" "$REDIS_MASTER_PORT_NUMBER" 2"

    add_replica() {
      if [[ "$1" != "$REDIS_MASTER_HOST" ]]; then
        sentinel_conf_add "sentinel known-replica mymaster $1 6379"
      fi
    }
    exec redis-server /opt/bitnami/redis-sentinel/etc/sentinel.conf --sentinel
---
# Source: stackstorm-ha/charts/redis/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: stackstorm-redis
  namespace: "stackstorm"
  labels:
    app: redis
    chart: redis-12.3.2
    heritage: Helm
    release: stackstorm
data:
  redis.conf: |-
    # User-supplied configuration:
    # Enable AOF https://redis.io/topics/persistence#append-only-file
    appendonly yes
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
  master.conf: |-
    dir /data
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
  replica.conf: |-
    dir /data
    slave-read-only yes
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
  sentinel.conf: |-
    dir "/tmp"
    bind 0.0.0.0
    port 26379
    sentinel monitor mymaster stackstorm-redis-node-0.stackstorm-redis-headless.stackstorm.svc.cluster.local 6379 2
    sentinel down-after-milliseconds mymaster 60000
    sentinel failover-timeout mymaster 18000
    sentinel parallel-syncs mymaster 1
---
# Source: stackstorm-ha/charts/redis/templates/health-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: stackstorm-redis-health
  namespace: "stackstorm"
  labels:
    app: redis
    chart: redis-12.3.2
    heritage: Helm
    release: stackstorm
data:
  ping_readiness_local.sh: |-
    #!/bin/bash
    export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 3 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_local.sh: |-
    #!/bin/bash
    export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 3 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$response" != "PONG" ] && [ "$response" != "LOADING Redis is loading the dataset in memory" ]; then
      echo "$response"
      exit 1
    fi
  ping_sentinel.sh: |-
    #!/bin/bash
    export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 3 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_SENTINEL_PORT \
        ping
    )
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  parse_sentinels.awk: |-
    /ip/ {FOUND_IP=1}
    /port/ {FOUND_PORT=1}
    /runid/ {FOUND_RUNID=1}
    !/ip|port|runid/ {
      if (FOUND_IP==1) {
        IP=$1; FOUND_IP=0;
      }
      else if (FOUND_PORT==1) {
        PORT=$1;
        FOUND_PORT=0;
      } else if (FOUND_RUNID==1) {
        printf "\nsentinel known-sentinel mymaster %s %s %s", IP, PORT, $0; FOUND_RUNID=0;
      }
    }
  ping_readiness_master.sh: |-
    #!/bin/bash
    export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 3 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_master.sh: |-
    #!/bin/bash
    export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 3 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$response" != "PONG" ] && [ "$response" != "LOADING Redis is loading the dataset in memory" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_readiness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_readiness_master.sh" $1 || exit_status=$?
    exit $exit_status
  ping_liveness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_liveness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_liveness_master.sh" $1 || exit_status=$?
    exit $exit_status
---
# Source: stackstorm-ha/templates/configmaps_packs.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: stackstorm-st2-pack-configs
  annotations:
    description: Custom StackStorm pack configs, shipped in '/opt/stackstorm/configs/'
  labels:
    app: st2
    tier: backend
    vendor: stackstorm
    chart: stackstorm-ha-0.60.0
    release: stackstorm
    heritage: Helm
data:
  core.yaml: |
    ---
    # example core pack config yaml
---
# Source: stackstorm-ha/templates/configmaps_st2-conf.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: stackstorm-st2-config
  annotations:
    description: Custom StackStorm config which will apply settings on top of default st2.conf
  labels:
    app: st2
    tier: backend
    vendor: stackstorm
    chart: stackstorm-ha-0.60.0
    release: stackstorm
    heritage: Helm
data:
  # TODO: Bundle DB/MQ login secrets in dynamic ENV-based st2.secrets.conf, leave custom user-defined settings for st2.user.conf (?)
  # Docker/K8s-based st2 config file used for templating service names and common overrides on top of original st2.conf.
  # The order of merging: st2.conf < st2.docker.conf < st2.user.conf
  st2.docker.conf: |
    [auth]
    api_url = http://stackstorm-st2api:9101/
    [coordination]
    url = redis://stackstorm-redis-node-0.stackstorm-redis-headless:26379?sentinel=mymaster&sentinel_fallback=stackstorm-redis-node-1.stackstorm-redis-headless:26379&sentinel_fallback=stackstorm-redis-node-2.stackstorm-redis-headless:26379
    [messaging]
    url = amqp://admin:9jS+w1u07NbHtZke1m+jW4Cj@stackstorm-rabbitmq:5672/
    [database]
    host = mongodb://stackstorm-mongodb-0.stackstorm-mongodb-headless,stackstorm-mongodb-1.stackstorm-mongodb-headless,stackstorm-mongodb-2.stackstorm-mongodb-headless/st2?authSource=st2&replicaSet=rs0
    username = st2-admin
    password = XeL5Rxwj7F0Wt43tFZVTN7H8Sg5XDHmK
    db_name = st2
    port = 27017

  # User-defined st2 config with custom settings applied on top of everything else.
  # The order of merging: st2.conf < st2.docker.conf < st2.user.conf
  st2.user.conf: |
    [api]
    allow_origin = '*'
---
# Source: stackstorm-ha/templates/configmaps_st2-urls.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: stackstorm-st2-urls
  annotations:
    description: StackStorm service URLs, used across entire st2 cluster
  labels:
    app: st2
    tier: backend
    vendor: stackstorm
    chart: stackstorm-ha-0.60.0
    release: stackstorm
    heritage: Helm
data:
  ST2_AUTH_URL: http://stackstorm-st2auth:9100/
  ST2_API_URL: http://stackstorm-st2api:9101/
  ST2_STREAM_URL: http://stackstorm-st2stream:9102/
---
# Source: stackstorm-ha/templates/tests/st2tests-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: stackstorm-st2tests
  labels:
    app: st2tests
    tier: tests
    vendor: stackstorm
    chart: stackstorm-ha-0.60.0
    release: stackstorm
    heritage: Helm
data:
  st2tests.sh: |
    #!/usr/bin/env bats
    
    load "${BATS_HELPERS_DIR}/bats-support/load.bash"
    load "${BATS_HELPERS_DIR}/bats-assert/load.bash"
    load "${BATS_HELPERS_DIR}/bats-file/load.bash"
    
    @test 'st2 version deployed and python env are as expected' {
      run st2 --version
      assert_success
      # st2 3.3dev (9ea417346), on Python 3.6.9
      assert_line --partial "st2 ${ST2_VERSION}"
      assert_line --partial 'on Python 3.6.9'
    }
    
    @test 'ST2_AUTH_URL service endpoint is accessible and working' {
      run curl -v ${ST2_API_URL}
      assert_line --partial 'Content-Type: application/json'
      assert_line --partial 'St2-Api-Key'
    }
    
    @test 'ST2_API_URL service endpoint is accessible and working' {
      run curl -v ${ST2_API_URL}
      assert_line --partial 'Content-Type: application/json'
      assert_line --partial 'St2-Api-Key'
    }
    
    @test 'ST2_STREAM_URL service endpoint is accessible and working' {
      run curl -v ${ST2_API_URL}
      assert_line --partial 'Content-Type: application/json'
      assert_line --partial 'St2-Api-Key'
    }
    
    @test 'st2 user can log in with auth credentials' {
      run st2 login ${ST2_AUTH_USERNAME} --password ${ST2_AUTH_PASSWORD} -w
      assert_success
      assert_line "Logged in as ${ST2_AUTH_USERNAME}"
      assert_file_exist ~/.st2/config
    }
    
    @test 'st2 core pack is installed and loaded' {
      run st2 action list --pack=core
      assert_success
      assert_line --partial 'core.local'
    }
    
    @test "can execute simple st2 action 'core.local'" {
      run st2 run core.local cmd=id
      assert_success
      assert_line --partial 'return_code: 0'
      assert_line --partial "stderr: ''"
      assert_line --partial 'stdout: uid=1000(stanley) gid=1000(stanley) groups=1000(stanley)'
      assert_line --partial 'succeeded: true'
    }
    
    @test 'st2 chatops core rule is loaded' {
      run st2 rule list
      assert_success
      assert_line --partial 'chatops.notify'
    }
    
    @test 'st2 key/value operations are functional' {
      run st2 key set foo bar
      assert_success
    
      run st2 key get foo
      assert_success
      assert_line --partial 'bar'
    
      run st2 key delete foo
      assert_line --partial '"foo" has been successfully deleted'
      assert_success
    
      run st2 key get foo
      assert_line --partial '"foo" is not found'
      assert_failure
    }
    
    @test 'RBAC is loaded and enabled' {
      if [ $ST2_RBAC_ENABLED != "true" ]; then
        skip "disabled in Helm values"
      fi
    
      run st2 whoami
      assert_success
      assert_output --regexp 'RBAC:\s+ - Enabled: True'
      assert_line --partial 'Roles: system_admin'
    }
---
# Source: stackstorm-ha/charts/rabbitmq/templates/role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: stackstorm-rabbitmq-endpoint-reader
  namespace: "stackstorm"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-8.0.2
    app.kubernetes.io/instance: stackstorm
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create"]
---
# Source: stackstorm-ha/charts/rabbitmq/templates/rolebinding.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: stackstorm-rabbitmq-endpoint-reader
  namespace: "stackstorm"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-8.0.2
    app.kubernetes.io/instance: stackstorm
    app.kubernetes.io/managed-by: Helm
subjects:
  - kind: ServiceAccount
    name: stackstorm-rabbitmq
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: stackstorm-rabbitmq-endpoint-reader
---
# Source: stackstorm-ha/charts/mongodb/templates/replicaset/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: stackstorm-mongodb-headless
  namespace: stackstorm
  labels:
    app.kubernetes.io/name: mongodb
    helm.sh/chart: mongodb-10.0.1
    app.kubernetes.io/instance: stackstorm
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: mongodb
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: mongodb
      port: 27017
      targetPort: mongodb
  selector:
    app.kubernetes.io/name: mongodb
    app.kubernetes.io/instance: stackstorm
    app.kubernetes.io/component: mongodb
---
# Source: stackstorm-ha/charts/rabbitmq/templates/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: stackstorm-rabbitmq-headless
  namespace: "stackstorm"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-8.0.2
    app.kubernetes.io/instance: stackstorm
    app.kubernetes.io/managed-by: Helm
spec:
  clusterIP: None
  ports:
    - name: epmd
      port: 4369
      targetPort: epmd
    - name: amqp
      port: 5672
      targetPort: amqp
    - name: dist
      port: 25672
      targetPort: dist
    - name: http-stats
      port: 15672
      targetPort: stats
  selector: 
    app.kubernetes.io/name: rabbitmq
    app.kubernetes.io/instance: stackstorm
---
# Source: stackstorm-ha/charts/rabbitmq/templates/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: stackstorm-rabbitmq
  namespace: "stackstorm"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-8.0.2
    app.kubernetes.io/instance: stackstorm
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: amqp
      port: 5672
      targetPort: amqp
      nodePort: null
    - name: epmd
      port: 4369
      targetPort: epmd
      nodePort: null
    - name: dist
      port: 25672
      targetPort: dist
      nodePort: null
    - name: http-stats
      port: 15672
      targetPort: stats
      nodePort: null
  selector: 
    app.kubernetes.io/name: rabbitmq
    app.kubernetes.io/instance: stackstorm
---
# Source: stackstorm-ha/charts/redis/templates/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: stackstorm-redis-headless
  namespace: "stackstorm"
  labels:
    app: redis
    chart: redis-12.3.2
    release: stackstorm
    heritage: Helm
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: redis
      port: 6379
      targetPort: redis
    - name: redis-sentinel
      port: 26379
      targetPort: redis-sentinel
  selector:
    app: redis
    release: stackstorm
---
# Source: stackstorm-ha/charts/redis/templates/redis-with-sentinel-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: stackstorm-redis
  namespace: "stackstorm"
  labels:
    app: redis
    chart: redis-12.3.2
    release: stackstorm
    heritage: Helm
spec:
  type: ClusterIP
  
  ports:
    - name: redis
      port: 6379
      targetPort: redis
    - name: redis-sentinel
      port: 26379
      targetPort: redis-sentinel
  selector:
    app: redis
    release: stackstorm
---
# Source: stackstorm-ha/templates/services.yaml
kind: Service
apiVersion: v1
metadata:
  name: stackstorm-st2auth
  annotations:
    description: StackStorm st2auth - all authentication is managed by this service.
  labels:
    app: st2auth
    tier: backend
    vendor: stackstorm
    chart: stackstorm-ha-0.60.0
    release: stackstorm
    heritage: Helm
spec:
  selector:
    app: st2auth
    release: stackstorm
  type: ClusterIP
  ports:
  - protocol: TCP
    port: 9100
---
# Source: stackstorm-ha/templates/services.yaml
kind: Service
apiVersion: v1
metadata:
  name: stackstorm-st2api
  annotations:
    description: StackStorm st2api - service hosts the REST API endpoints that serve requests from WebUI, CLI, ChatOps and other st2 services.
  labels:
    app: st2api
    tier: backend
    vendor: stackstorm
    chart: stackstorm-ha-0.60.0
    release: stackstorm
    heritage: Helm
spec:
  selector:
    app: st2api
    release: stackstorm
  type: ClusterIP
  ports:
  - protocol: TCP
    port: 9101
---
# Source: stackstorm-ha/templates/services.yaml
kind: Service
apiVersion: v1
metadata:
  name: stackstorm-st2stream
  annotations:
    description: StackStorm st2stream - exposes a server-sent event stream, used by the clients like WebUI and ChatOps to receive update from the st2stream server.
  labels:
    app: st2stream
    tier: backend
    vendor: stackstorm
    chart: stackstorm-ha-0.60.0
    release: stackstorm
    heritage: Helm
spec:
  selector:
    app: st2stream
    release: stackstorm
  type: ClusterIP
  ports:
  - protocol: TCP
    port: 9102
---
# Source: stackstorm-ha/templates/services.yaml
kind: Service
apiVersion: v1
metadata:
  name: stackstorm-st2web
  annotations:
    description: StackStorm st2web, - an admin Web UI and main entry point for external API requests
  labels:
    app: st2web
    tier: frontend
    vendor: stackstorm
    chart: stackstorm-ha-0.60.0
    release: stackstorm
    heritage: Helm
spec:
  selector:
    app: st2web
    release: stackstorm
  type: NodePort
  ports:
  - protocol: TCP
    port: 80
---
# Source: stackstorm-ha/templates/deployments.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: stackstorm-st2auth
  labels:
    app: st2auth
    tier: backend
    vendor: stackstorm
    chart: stackstorm-ha-0.60.0
    release: stackstorm
    heritage: Helm
spec:
  selector:
    matchLabels:
      app: st2auth
      release: stackstorm
  # https://docs.stackstorm.com/reference/ha.html#st2auth
  # Multiple st2auth processes can be behind a load balancer in an active-active configuration.
  replicas: 2
  template:
    metadata:
      labels:
        app: st2auth
        tier: backend
        vendor: stackstorm
        chart: stackstorm-ha-0.60.0
        release: stackstorm
        heritage: Helm
      annotations:
        checksum/config: 2cecfe3bc433d799634db9c906275d6ffa3ff5eb673367d9046a0acd70b4a77d
        checksum/auth: 2a5ed78947ad6a3bf26036c76f41efcb2082d4e0c17b1cae9b3a2c7230443ca7
    spec:
      imagePullSecrets:
      initContainers:
            
      - name: wait-for-db
        image: busybox:1.28
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 stackstorm-mongodb-headless 27017 && echo mongodb ok;
              do 
                echo 'Waiting for MongoDB Connection...'
                sleep 2;
            done
            
      - name: wait-for-queue
        image: busybox:1.28
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 stackstorm-rabbitmq 5672 && echo rabbitmq ok;
              do
                echo 'Waiting for RabbitMQ Connection...'
                sleep 2;
            done
      # Sidecar container for generating .htpasswd with st2 username & password pair and sharing produced file with the main st2auth container
      - name: generate-htpasswd
        image: "stackstorm/st2auth:3.5dev"
        imagePullPolicy: IfNotPresent
        env:
        - name: ST2_AUTH_USERNAME
          valueFrom:
            secretKeyRef:
              name: stackstorm-st2-auth
              key: username
        - name: ST2_AUTH_PASSWORD
          valueFrom:
            secretKeyRef:
              name: stackstorm-st2-auth
              key: password
        volumeMounts:
        - name: htpasswd-vol
          mountPath: /tmp/st2
        command:
          - 'sh'
          - '-ec'
          - printf "${ST2_AUTH_USERNAME}:$(openssl passwd -apr1 "${ST2_AUTH_PASSWORD}")\n" > /tmp/st2/htpasswd
      containers:
      - name: st2auth
        image: "stackstorm/st2auth:3.5dev"
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 9100
        # TODO: Add liveness/readiness probes (#3)
        #livenessProbe:
        #readinessProbe:
        envFrom:
        - configMapRef:
            name: stackstorm-st2-urls
        volumeMounts:
        - name: st2-config-vol
          mountPath: /etc/st2/st2.docker.conf
          subPath: st2.docker.conf
        - name: st2-config-vol
          mountPath: /etc/st2/st2.user.conf
          subPath: st2.user.conf
        - name: htpasswd-vol
          mountPath: /etc/st2/htpasswd
          subPath: htpasswd
          readOnly: true
        resources:
          requests:
            cpu: 50m
            memory: 85Mi
      volumes:
        - name: st2-config-vol
          configMap:
            name: stackstorm-st2-config
        - name: htpasswd-vol
          emptyDir:
            medium: Memory
---
# Source: stackstorm-ha/templates/deployments.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: stackstorm-st2api
  labels:
    app: st2api
    tier: backend
    vendor: stackstorm
    chart: stackstorm-ha-0.60.0
    release: stackstorm
    heritage: Helm
spec:
  selector:
    matchLabels:
      app: st2api
      release: stackstorm
  # https://docs.stackstorm.com/reference/ha.html#st2api
  # Multiple st2api process can be behind a load balancer in an active-active configuration.
  replicas: 2
  template:
    metadata:
      labels:
        app: st2api
        tier: backend
        vendor: stackstorm
        chart: stackstorm-ha-0.60.0
        release: stackstorm
        heritage: Helm
      annotations:
        checksum/config: 2cecfe3bc433d799634db9c906275d6ffa3ff5eb673367d9046a0acd70b4a77d
        checksum/datastore-key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
    spec:
      imagePullSecrets:
      initContainers:
            
      - name: wait-for-db
        image: busybox:1.28
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 stackstorm-mongodb-headless 27017 && echo mongodb ok;
              do 
                echo 'Waiting for MongoDB Connection...'
                sleep 2;
            done
            
      - name: wait-for-queue
        image: busybox:1.28
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 stackstorm-rabbitmq 5672 && echo rabbitmq ok;
              do
                echo 'Waiting for RabbitMQ Connection...'
                sleep 2;
            done
      containers:
      - name: st2api
        image: "stackstorm/st2api:3.5dev"
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 9101
        # TODO: Add liveness/readiness probes (#3)
        #livenessProbe:
        #readinessProbe:
        envFrom:
        - configMapRef:
            name: stackstorm-st2-urls
        volumeMounts:
        - name: st2-config-vol
          mountPath: /etc/st2/st2.docker.conf
          subPath: st2.docker.conf
        - name: st2-config-vol
          mountPath: /etc/st2/st2.user.conf
          subPath: st2.user.conf
        resources:
          requests:
            cpu: 25m
            memory: 150Mi
      volumes:
        - name: st2-config-vol
          configMap:
            name: stackstorm-st2-config
---
# Source: stackstorm-ha/templates/deployments.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: stackstorm-st2stream
  labels:
    app: st2stream
    tier: backend
    vendor: stackstorm
    chart: stackstorm-ha-0.60.0
    release: stackstorm
    heritage: Helm
spec:
  selector:
    matchLabels:
      app: st2stream
      release: stackstorm
  # https://docs.stackstorm.com/reference/ha.html#st2stream
  # Multiple st2stream process can be behind a load balancer in an active-active configuration.
  replicas: 2
  template:
    metadata:
      labels:
        app: st2stream
        tier: backend
        vendor: stackstorm
        chart: stackstorm-ha-0.60.0
        release: stackstorm
        heritage: Helm
      annotations:
        checksum/config: 2cecfe3bc433d799634db9c906275d6ffa3ff5eb673367d9046a0acd70b4a77d
    spec:
      imagePullSecrets:
      initContainers:
            
      - name: wait-for-db
        image: busybox:1.28
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 stackstorm-mongodb-headless 27017 && echo mongodb ok;
              do 
                echo 'Waiting for MongoDB Connection...'
                sleep 2;
            done
            
      - name: wait-for-queue
        image: busybox:1.28
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 stackstorm-rabbitmq 5672 && echo rabbitmq ok;
              do
                echo 'Waiting for RabbitMQ Connection...'
                sleep 2;
            done
      containers:
      - name: st2stream
        image: "stackstorm/st2stream:3.5dev"
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 9102
        # TODO: Add liveness/readiness probes (#3)
        #livenessProbe:
        #readinessProbe:
        envFrom:
        - configMapRef:
            name: stackstorm-st2-urls
        volumeMounts:
        - name: st2-config-vol
          mountPath: /etc/st2/st2.docker.conf
          subPath: st2.docker.conf
        - name: st2-config-vol
          mountPath: /etc/st2/st2.user.conf
          subPath: st2.user.conf
        resources:
          requests:
            cpu: 50m
            memory: 100Mi
      volumes:
        - name: st2-config-vol
          configMap:
            name: stackstorm-st2-config
---
# Source: stackstorm-ha/templates/deployments.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: stackstorm-st2web
  labels:
    app: st2web
    tier: frontend
    vendor: stackstorm
    chart: stackstorm-ha-0.60.0
    release: stackstorm
    heritage: Helm
spec:
  selector:
    matchLabels:
      app: st2web
      release: stackstorm
  replicas: 2
  template:
    metadata:
      labels:
        app: st2web
        tier: frontend
        vendor: stackstorm
        chart: stackstorm-ha-0.60.0
        release: stackstorm
        heritage: Helm
      annotations:
        checksum/config: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
    spec:
      imagePullSecrets:
      containers:
      - name: st2web
        image: "stackstorm/st2web:3.5dev"
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
        # Probe to check if app is running. Failure will lead to a pod restart.
        livenessProbe:
          httpGet:
            scheme: HTTP
            path: /
            port: 80
          initialDelaySeconds: 1
        # Probe to check if app is ready to serve traffic. Failure will lead to temp stop serving traffic.
        # TODO: Failing to add readinessProbe, since st2 requires authorization (401) and we don't have `/healthz` endpoints yet (https://github.com/StackStorm/st2/issues/4020)
#        readinessProbe:
#          httpGet:
#            # Probes can't check several endpoints, - this should be implemented on app side (@see https://www.ianlewis.org/en/using-kubernetes-health-checks)
#            # Also multiple liveness checks are not available (@see https://github.com/kubernetes/kubernetes/issues/37218)
#            # So checking ST2_API only
#            scheme: HTTPS
#            path: /api/
#            port: 443
#          initialDelaySeconds: 3
        envFrom:
        - configMapRef:
            name: stackstorm-st2-urls
            optional: true
        volumeMounts: []
        resources:
          limits:
            memory: 100Mi
          requests:
            cpu: 50m
            memory: 25Mi
      volumes: []
---
# Source: stackstorm-ha/templates/deployments.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: stackstorm-st2rulesengine
  labels:
    app: st2rulesengine
    tier: backend
    vendor: stackstorm
    chart: stackstorm-ha-0.60.0
    release: stackstorm
    heritage: Helm
spec:
  selector:
    matchLabels:
      app: st2rulesengine
      release: stackstorm
  # https://docs.stackstorm.com/reference/ha.html#st2rulesengine
  # Multiple st2rulesengine processes can run in active-active with only connections to MongoDB and RabbitMQ. All these will share the TriggerInstance load and naturally pick up more work if one or more of the processes becomes unavailable.
  replicas: 2
  template:
    metadata:
      labels:
        app: st2rulesengine
        tier: backend
        vendor: stackstorm
        chart: stackstorm-ha-0.60.0
        release: stackstorm
        heritage: Helm
      annotations:
        checksum/config: 2cecfe3bc433d799634db9c906275d6ffa3ff5eb673367d9046a0acd70b4a77d
    spec:
      imagePullSecrets:
      initContainers:
            
      - name: wait-for-db
        image: busybox:1.28
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 stackstorm-mongodb-headless 27017 && echo mongodb ok;
              do 
                echo 'Waiting for MongoDB Connection...'
                sleep 2;
            done
            
      - name: wait-for-queue
        image: busybox:1.28
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 stackstorm-rabbitmq 5672 && echo rabbitmq ok;
              do
                echo 'Waiting for RabbitMQ Connection...'
                sleep 2;
            done
      containers:
      - name: st2rulesengine
        image: "stackstorm/st2rulesengine:3.5dev"
        imagePullPolicy: IfNotPresent
        # TODO: Add liveness/readiness probes (#3)
        #livenessProbe:
        #readinessProbe:
        envFrom:
        - configMapRef:
            name: stackstorm-st2-urls
        volumeMounts:
        - name: st2-config-vol
          mountPath: /etc/st2/st2.docker.conf
          subPath: st2.docker.conf
        - name: st2-config-vol
          mountPath: /etc/st2/st2.user.conf
          subPath: st2.user.conf
        resources:
          requests:
            cpu: 25m
            memory: 75Mi
      volumes:
        - name: st2-config-vol
          configMap:
            name: stackstorm-st2-config
---
# Source: stackstorm-ha/templates/deployments.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: stackstorm-st2timersengine
  labels:
    app: st2timersengine
    tier: backend
    vendor: stackstorm
    chart: stackstorm-ha-0.60.0
    release: stackstorm
    heritage: Helm
spec:
  selector:
    matchLabels:
      app: st2timersengine
      release: stackstorm
  # https://docs.stackstorm.com/reference/ha.html#st2timersengine
  # Only single replica is created as timersengine can't work in active-active mode at the moment and it relies on
  # K8s failover/reschedule capabilities to address cases when the process fails.
  replicas: 1
  template:
    metadata:
      labels:
        app: st2timersengine
        tier: backend
        vendor: stackstorm
        chart: stackstorm-ha-0.60.0
        release: stackstorm
        heritage: Helm
      annotations:
        checksum/config: 2cecfe3bc433d799634db9c906275d6ffa3ff5eb673367d9046a0acd70b4a77d
    spec:
      imagePullSecrets:
      initContainers:
            
      - name: wait-for-db
        image: busybox:1.28
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 stackstorm-mongodb-headless 27017 && echo mongodb ok;
              do 
                echo 'Waiting for MongoDB Connection...'
                sleep 2;
            done
            
      - name: wait-for-queue
        image: busybox:1.28
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 stackstorm-rabbitmq 5672 && echo rabbitmq ok;
              do
                echo 'Waiting for RabbitMQ Connection...'
                sleep 2;
            done
      containers:
      - name: st2timersengine
        image: "stackstorm/st2timersengine:3.5dev"
        imagePullPolicy: IfNotPresent
        # TODO: Add liveness/readiness probes (#3)
        #livenessProbe:
        #readinessProbe:
        envFrom:
        - configMapRef:
            name: stackstorm-st2-urls
        volumeMounts:
        - name: st2-config-vol
          mountPath: /etc/st2/st2.docker.conf
          subPath: st2.docker.conf
        - name: st2-config-vol
          mountPath: /etc/st2/st2.user.conf
          subPath: st2.user.conf
        resources:
          requests:
            cpu: 10m
            memory: 75Mi
      volumes:
        - name: st2-config-vol
          configMap:
            name: stackstorm-st2-config
---
# Source: stackstorm-ha/templates/deployments.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: stackstorm-st2workflowengine
  labels:
    app: st2workflowengine
    tier: backend
    vendor: stackstorm
    chart: stackstorm-ha-0.60.0
    release: stackstorm
    heritage: Helm
spec:
  selector:
    matchLabels:
      app: st2workflowengine
      release: stackstorm
  # https://docs.stackstorm.com/reference/ha.html#st2workflowengine
  # Multiple st2workflowengine processes can run in active-active mode and will share the load and pick up more work if one or more of the processes become available.
  replicas: 2
  template:
    metadata:
      labels:
        app: st2workflowengine
        tier: backend
        vendor: stackstorm
        chart: stackstorm-ha-0.60.0
        release: stackstorm
        heritage: Helm
      annotations:
        checksum/config: 2cecfe3bc433d799634db9c906275d6ffa3ff5eb673367d9046a0acd70b4a77d
        checksum/datastore-key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
    spec:
      imagePullSecrets:
      initContainers:
            
      - name: wait-for-db
        image: busybox:1.28
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 stackstorm-mongodb-headless 27017 && echo mongodb ok;
              do 
                echo 'Waiting for MongoDB Connection...'
                sleep 2;
            done
            
      - name: wait-for-queue
        image: busybox:1.28
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 stackstorm-rabbitmq 5672 && echo rabbitmq ok;
              do
                echo 'Waiting for RabbitMQ Connection...'
                sleep 2;
            done
      containers:
      - name: st2workflowengine
        image: "stackstorm/st2workflowengine:3.5dev"
        imagePullPolicy: IfNotPresent
        # TODO: Add liveness/readiness probes (#3)
        #livenessProbe:
        #readinessProbe:
        envFrom:
        - configMapRef:
            name: stackstorm-st2-urls
        volumeMounts:
        - name: st2-config-vol
          mountPath: /etc/st2/st2.docker.conf
          subPath: st2.docker.conf
        - name: st2-config-vol
          mountPath: /etc/st2/st2.user.conf
          subPath: st2.user.conf
        resources:
          requests:
            cpu: 100m
            memory: 200Mi
      volumes:
        - name: st2-config-vol
          configMap:
            name: stackstorm-st2-config
---
# Source: stackstorm-ha/templates/deployments.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: stackstorm-st2scheduler
  labels:
    app: st2scheduler
    tier: backend
    vendor: stackstorm
    chart: stackstorm-ha-0.60.0
    release: stackstorm
    heritage: Helm
spec:
  selector:
    matchLabels:
      app: st2scheduler
      release: stackstorm
  # https://docs.stackstorm.com/reference/ha.html#st2scheduler
  replicas: 2
  template:
    metadata:
      labels:
        app: st2scheduler
        tier: backend
        vendor: stackstorm
        chart: stackstorm-ha-0.60.0
        release: stackstorm
        heritage: Helm
      annotations:
        checksum/config: 2cecfe3bc433d799634db9c906275d6ffa3ff5eb673367d9046a0acd70b4a77d
        checksum/datastore-key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
    spec:
      imagePullSecrets:
      initContainers:
            
      - name: wait-for-db
        image: busybox:1.28
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 stackstorm-mongodb-headless 27017 && echo mongodb ok;
              do 
                echo 'Waiting for MongoDB Connection...'
                sleep 2;
            done
            
      - name: wait-for-queue
        image: busybox:1.28
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 stackstorm-rabbitmq 5672 && echo rabbitmq ok;
              do
                echo 'Waiting for RabbitMQ Connection...'
                sleep 2;
            done
      containers:
      - name: st2scheduler
        image: "stackstorm/st2scheduler:3.5dev"
        imagePullPolicy: IfNotPresent
        # TODO: Add liveness/readiness probes (#3)
        #livenessProbe:
        #readinessProbe:
        envFrom:
        - configMapRef:
            name: stackstorm-st2-urls
        volumeMounts:
        - name: st2-config-vol
          mountPath: /etc/st2/st2.docker.conf
          subPath: st2.docker.conf
        - name: st2-config-vol
          mountPath: /etc/st2/st2.user.conf
          subPath: st2.user.conf
        resources:
          requests:
            cpu: 50m
            memory: 75Mi
      volumes:
        - name: st2-config-vol
          configMap:
            name: stackstorm-st2-config
---
# Source: stackstorm-ha/templates/deployments.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: stackstorm-st2notifier
  labels:
    app: st2notifier
    tier: backend
    vendor: stackstorm
    chart: stackstorm-ha-0.60.0
    release: stackstorm
    heritage: Helm
spec:
  selector:
    matchLabels:
      app: st2notifier
      release: stackstorm
  # https://docs.stackstorm.com/reference/ha.html#st2notifier
  # st2notifier runs in active-active mode and requires for that coordination backend like Redis or Zookeeper
  replicas: 2
  template:
    metadata:
      labels:
        app: st2notifier
        tier: backend
        vendor: stackstorm
        chart: stackstorm-ha-0.60.0
        release: stackstorm
        heritage: Helm
      annotations:
        checksum/config: 2cecfe3bc433d799634db9c906275d6ffa3ff5eb673367d9046a0acd70b4a77d
    spec:
      imagePullSecrets:
      initContainers:
            
      - name: wait-for-db
        image: busybox:1.28
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 stackstorm-mongodb-headless 27017 && echo mongodb ok;
              do 
                echo 'Waiting for MongoDB Connection...'
                sleep 2;
            done
            
      - name: wait-for-queue
        image: busybox:1.28
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 stackstorm-rabbitmq 5672 && echo rabbitmq ok;
              do
                echo 'Waiting for RabbitMQ Connection...'
                sleep 2;
            done
      containers:
      - name: st2notifier
        image: "stackstorm/st2notifier:3.5dev"
        imagePullPolicy: IfNotPresent
        # TODO: Add liveness/readiness probes (#3)
        #livenessProbe:
        #readinessProbe:
        envFrom:
        - configMapRef:
            name: stackstorm-st2-urls
        volumeMounts:
        - name: st2-config-vol
          mountPath: /etc/st2/st2.docker.conf
          subPath: st2.docker.conf
        - name: st2-config-vol
          mountPath: /etc/st2/st2.user.conf
          subPath: st2.user.conf
        resources:
          requests:
            cpu: 50m
            memory: 75Mi
      volumes:
        - name: st2-config-vol
          configMap:
            name: stackstorm-st2-config
---
# Source: stackstorm-ha/templates/deployments.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: stackstorm-st2sensorcontainer
  labels:
    app: st2sensorcontainer
    tier: backend
    vendor: stackstorm
    chart: stackstorm-ha-0.60.0
    release: stackstorm
    heritage: Helm
spec:
  selector:
    matchLabels:
      app: st2sensorcontainer
      release: stackstorm
  # https://docs.stackstorm.com/reference/ha.html#st2sensorcontainer
  # It is possible to run st2sensorcontainer in HA mode by running one process on each compute instance. Each sensor node needs to be
  # provided with proper partition information to share work with other sensor nodes so that the same sensor does not run on different nodes.
  # See Partitioning Sensors for information on how to partition sensors.
  replicas: 1
  template:
    metadata:
      labels:
        app: st2sensorcontainer
        tier: backend
        vendor: stackstorm
        chart: stackstorm-ha-0.60.0
        release: stackstorm
        heritage: Helm
      annotations:
        checksum/config: 2cecfe3bc433d799634db9c906275d6ffa3ff5eb673367d9046a0acd70b4a77d
        checksum/packs: f7ce2c2f8802015dab601c8f20e98b1657d48c26311bd956ac7b9e0250077051
        checksum/datastore-key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
    spec:
      imagePullSecrets:
      initContainers:
            
      - name: wait-for-db
        image: busybox:1.28
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 stackstorm-mongodb-headless 27017 && echo mongodb ok;
              do 
                echo 'Waiting for MongoDB Connection...'
                sleep 2;
            done
            
      - name: wait-for-queue
        image: busybox:1.28
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 stackstorm-rabbitmq 5672 && echo rabbitmq ok;
              do
                echo 'Waiting for RabbitMQ Connection...'
                sleep 2;
            done
      containers:
      - name: st2sensorcontainer
        image: "stackstorm/st2sensorcontainer:3.5dev"
        imagePullPolicy: IfNotPresent
        envFrom:
        - configMapRef:
            name: stackstorm-st2-urls
        volumeMounts:
        - name: st2-config-vol
          mountPath: /etc/st2/st2.docker.conf
          subPath: st2.docker.conf
        - name: st2-config-vol
          mountPath: /etc/st2/st2.user.conf
          subPath: st2.user.conf
        resources:
          requests:
            cpu: 50m
            memory: 100Mi
      volumes:
        - name: st2-config-vol
          configMap:
            name: stackstorm-st2-config
---
# Source: stackstorm-ha/templates/deployments.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: stackstorm-st2actionrunner
  labels:
    app: st2actionrunner
    tier: backend
    vendor: stackstorm
    chart: stackstorm-ha-0.60.0
    release: stackstorm
    heritage: Helm
spec:
  selector:
    matchLabels:
      app: st2actionrunner
      release: stackstorm
  # https://docs.stackstorm.com/reference/ha.html#st2actionrunner
  # Multiple st2actionrunner processes can run in active-active with only connections to MongoDB and RabbitMQ. Work gets naturally
  # distributed across runners via RabbitMQ. Adding more st2actionrunner processes increases the ability of StackStorm to execute actions.
  replicas: 5
  template:
    metadata:
      labels:
        app: st2actionrunner
        tier: backend
        vendor: stackstorm
        chart: stackstorm-ha-0.60.0
        release: stackstorm
        heritage: Helm
      annotations:
        checksum/config: 2cecfe3bc433d799634db9c906275d6ffa3ff5eb673367d9046a0acd70b4a77d
        checksum/ssh: fccd3cffc6939157ebf1ebfd1b289b23ce28bae718ff79b907f5fdb920b186ad
        checksum/datastore-key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
    spec:
      imagePullSecrets:
      initContainers:
            
      - name: wait-for-db
        image: busybox:1.28
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 stackstorm-mongodb-headless 27017 && echo mongodb ok;
              do 
                echo 'Waiting for MongoDB Connection...'
                sleep 2;
            done
            
      - name: wait-for-queue
        image: busybox:1.28
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 stackstorm-rabbitmq 5672 && echo rabbitmq ok;
              do
                echo 'Waiting for RabbitMQ Connection...'
                sleep 2;
            done
      containers:
      - name: st2actionrunner
        image: "stackstorm/st2actionrunner:3.5dev"
        imagePullPolicy: IfNotPresent
        # TODO: Add liveness/readiness probes (#3)
        #livenessProbe:
        #readinessProbe:
        envFrom:
        - configMapRef:
            name: stackstorm-st2-urls
        volumeMounts:
        - name: st2-config-vol
          mountPath: /etc/st2/st2.docker.conf
          subPath: st2.docker.conf
        - name: st2-config-vol
          mountPath: /etc/st2/st2.user.conf
          subPath: st2.user.conf
        - name: st2-ssh-key-vol
          mountPath: /home/stanley/.ssh/
          readOnly: true
        resources:
          requests:
            cpu: 75m
            memory: 200Mi
      volumes:
        - name: st2-config-vol
          configMap:
            name: stackstorm-st2-config
        - name: st2-ssh-key-vol
          secret:
            secretName: stackstorm-st2-ssh
            items:
            - key: private_key
              path: stanley_rsa
              # 0400 file permission
              mode: 256
---
# Source: stackstorm-ha/templates/deployments.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: stackstorm-st2garbagecollector
  labels:
    app: st2garbagecollector
    tier: backend
    vendor: stackstorm
    chart: stackstorm-ha-0.60.0
    release: stackstorm
    heritage: Helm
spec:
  selector:
    matchLabels:
      app: st2garbagecollector
      release: stackstorm
  # https://docs.stackstorm.com/reference/ha.html#st2garbagecollector
  # Having 1 st2garbagecollector unique replica is enough for periodic task like st2 history garbage collection
  replicas: 1
  template:
    metadata:
      labels:
        app: st2garbagecollector
        tier: backend
        vendor: stackstorm
        chart: stackstorm-ha-0.60.0
        release: stackstorm
        heritage: Helm
      annotations:
        checksum/config: 2cecfe3bc433d799634db9c906275d6ffa3ff5eb673367d9046a0acd70b4a77d
    spec:
      imagePullSecrets:
      initContainers:
            
      - name: wait-for-db
        image: busybox:1.28
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 stackstorm-mongodb-headless 27017 && echo mongodb ok;
              do 
                echo 'Waiting for MongoDB Connection...'
                sleep 2;
            done
            
      - name: wait-for-queue
        image: busybox:1.28
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 stackstorm-rabbitmq 5672 && echo rabbitmq ok;
              do
                echo 'Waiting for RabbitMQ Connection...'
                sleep 2;
            done
      containers:
      - name: st2garbagecollector
        image: "stackstorm/st2garbagecollector:3.5dev"
        imagePullPolicy: IfNotPresent
        # TODO: Add liveness/readiness probes (#3)
        #livenessProbe:
        #readinessProbe:
        envFrom:
        - configMapRef:
            name: stackstorm-st2-urls
        volumeMounts:
        - name: st2-config-vol
          mountPath: /etc/st2/st2.docker.conf
          subPath: st2.docker.conf
        - name: st2-config-vol
          mountPath: /etc/st2/st2.user.conf
          subPath: st2.user.conf
        resources:
          requests:
            cpu: 10m
            memory: 80Mi
      volumes:
        - name: st2-config-vol
          configMap:
            name: stackstorm-st2-config
---
# Source: stackstorm-ha/templates/deployments.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: stackstorm-st2client
  labels:
    app: st2client
    tier: backend
    vendor: stackstorm
    chart: stackstorm-ha-0.60.0
    release: stackstorm
    heritage: Helm
spec:
  selector:
    matchLabels:
      app: st2client
      release: stackstorm
  replicas: 1
  template:
    metadata:
      labels:
        app: st2client
        tier: backend
        vendor: stackstorm
        chart: stackstorm-ha-0.60.0
        release: stackstorm
        heritage: Helm
      annotations:
        checksum/config: 2cecfe3bc433d799634db9c906275d6ffa3ff5eb673367d9046a0acd70b4a77d
        checksum/rbac: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        checksum/packs: f7ce2c2f8802015dab601c8f20e98b1657d48c26311bd956ac7b9e0250077051
        checksum/auth: 2a5ed78947ad6a3bf26036c76f41efcb2082d4e0c17b1cae9b3a2c7230443ca7
        checksum/ssh: fccd3cffc6939157ebf1ebfd1b289b23ce28bae718ff79b907f5fdb920b186ad
        checksum/datastore-key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
    spec:
      imagePullSecrets:
      initContainers:
      # Sidecar container for generating st2client config with st2 username & password pair and sharing produced file with the main container
      - name: generate-st2client-config
        image: "stackstorm/st2actionrunner:3.5dev"
        imagePullPolicy: IfNotPresent
        envFrom:
        - configMapRef:
            name: stackstorm-st2-urls
        env:
        - name: ST2_AUTH_USERNAME
          valueFrom:
            secretKeyRef:
              name: stackstorm-st2-auth
              key: username
        - name: ST2_AUTH_PASSWORD
          valueFrom:
            secretKeyRef:
              name: stackstorm-st2-auth
              key: password
        volumeMounts:
        - name: st2client-config-vol
          mountPath: /root/.st2/
        # `st2 login` doesn't exit on failure correctly, use old methods instead. See bug: https://github.com/StackStorm/st2/issues/4338
        command:
          - 'sh'
          - '-ec'
          - |
            cat <<EOT > /root/.st2/config
            [credentials]
            username = ${ST2_AUTH_USERNAME}
            password = ${ST2_AUTH_PASSWORD}
            EOT
      containers:
      - name: st2client
        image: "stackstorm/st2actionrunner:3.5dev"
        imagePullPolicy: IfNotPresent
        env:
        - name: ST2CLIENT
          value: "1"
        envFrom:
        - configMapRef:
            name: stackstorm-st2-urls
        volumeMounts:
        - name: st2-config-vol
          mountPath: /etc/st2/st2.docker.conf
          subPath: st2.docker.conf
        - name: st2-config-vol
          mountPath: /etc/st2/st2.user.conf
          subPath: st2.user.conf 
        - name: st2-pack-configs-vol
          mountPath: /opt/stackstorm/configs/
        - name: st2client-config-vol
          mountPath: /root/.st2/
        - name: st2-ssh-key-vol
          mountPath: /home/stanley/.ssh/
          readOnly: true
        command:
          - 'bash'
          - '-ec'
          - 'while true; do sleep 999; done'
        resources:
          requests:
            memory: "5Mi"
            cpu: "5m"
      volumes:
        - name: st2-config-vol
          configMap:
            name: stackstorm-st2-config
        - name: st2-pack-configs-vol
          configMap:
            name: stackstorm-st2-pack-configs
        - name: st2client-config-vol
          emptyDir:
            medium: Memory
        - name: st2-ssh-key-vol
          secret:
            secretName: stackstorm-st2-ssh
            items:
            - key: private_key
              path: stanley_rsa
              # 0400 file permission
              mode: 256
---
# Source: stackstorm-ha/charts/mongodb/templates/replicaset/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: stackstorm-mongodb
  namespace: stackstorm
  labels:
    app.kubernetes.io/name: mongodb
    helm.sh/chart: mongodb-10.0.1
    app.kubernetes.io/instance: stackstorm
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: mongodb
spec:
  serviceName: stackstorm-mongodb-headless
  podManagementPolicy: OrderedReady
  replicas: 3
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: mongodb
      app.kubernetes.io/instance: stackstorm
      app.kubernetes.io/component: mongodb
  template:
    metadata:
      labels:
        app.kubernetes.io/name: mongodb
        helm.sh/chart: mongodb-10.0.1
        app.kubernetes.io/instance: stackstorm
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: mongodb
    spec:
      
      serviceAccountName: stackstorm-mongodb
      securityContext:
        fsGroup: 1001
        sysctls: []
      containers:
        - name: mongodb
          image: docker.io/bitnami/mongodb:4.0
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          command:
            - /scripts/setup.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: MY_POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: K8S_SERVICE_NAME
              value: "stackstorm-mongodb-headless"
            - name: MONGODB_INITIAL_PRIMARY_HOST
              value: "stackstorm-mongodb-0.$(K8S_SERVICE_NAME).$(MY_POD_NAMESPACE).svc.cluster.local"
            - name: MONGODB_REPLICA_SET_NAME
              value: "rs0"
            - name: MONGODB_ADVERTISED_HOSTNAME
              value: "$(MY_POD_NAME).$(K8S_SERVICE_NAME).$(MY_POD_NAMESPACE).svc.cluster.local"
            - name: MONGODB_USERNAME
              value: "st2-admin"
            - name: MONGODB_DATABASE
              value: "st2"
            - name: MONGODB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: stackstorm-mongodb
                  key: mongodb-password
            - name: MONGODB_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: stackstorm-mongodb
                  key: mongodb-root-password
            - name: MONGODB_REPLICA_SET_KEY
              valueFrom:
                secretKeyRef:
                  name: stackstorm-mongodb
                  key: mongodb-replica-set-key
            - name: ALLOW_EMPTY_PASSWORD
              value: "no"
            - name: MONGODB_SYSTEM_LOG_VERBOSITY
              value: "0"
            - name: MONGODB_DISABLE_SYSTEM_LOG
              value: "no"
            - name: MONGODB_ENABLE_IPV6
              value: "no"
            - name: MONGODB_ENABLE_DIRECTORY_PER_DB
              value: "no"
          ports:
            - containerPort: 27017
              name: mongodb
          livenessProbe:
            exec:
              command:
                - mongo
                - --eval
                - "db.adminCommand('ping')"
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            exec:
              command:
                - mongo
                - --eval
                - "db.adminCommand('ping')"
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: datadir
              mountPath: /bitnami/mongodb
              subPath: 
            - name: scripts
              mountPath: /scripts/setup.sh
              subPath: setup.sh
      volumes:
        - name: scripts
          configMap:
            name: stackstorm-mongodb-scripts
            defaultMode: 0755
  volumeClaimTemplates:
    - metadata:
        name: datadir
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: stackstorm-ha/charts/rabbitmq/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: stackstorm-rabbitmq
  namespace: "stackstorm"
  labels:
    app.kubernetes.io/name: rabbitmq
    helm.sh/chart: rabbitmq-8.0.2
    app.kubernetes.io/instance: stackstorm
    app.kubernetes.io/managed-by: Helm
spec:
  serviceName: stackstorm-rabbitmq-headless
  podManagementPolicy: OrderedReady
  replicas: 3
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: rabbitmq
      app.kubernetes.io/instance: stackstorm
  template:
    metadata:
      labels:
        app.kubernetes.io/name: rabbitmq
        helm.sh/chart: rabbitmq-8.0.2
        app.kubernetes.io/instance: stackstorm
        app.kubernetes.io/managed-by: Helm
      annotations:
        checksum/secret: c6145e8074ef7185820f93f5c56940dc7cec7e46acd445be115c06b050cf16bf
    spec:
      
      serviceAccountName: stackstorm-rabbitmq
      securityContext:
        fsGroup: 1001
        runAsUser: 1001
      terminationGracePeriodSeconds: 10
      containers:
        - name: rabbitmq
          image: docker.io/bitnami/rabbitmq:3.8.9-debian-10-r37
          imagePullPolicy: "IfNotPresent"
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: MY_POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: K8S_SERVICE_NAME
              value: "stackstorm-rabbitmq-headless"
            - name: K8S_ADDRESS_TYPE
              value: hostname
            - name: RABBITMQ_FORCE_BOOT
              value: "yes"
            - name: RABBITMQ_NODE_NAME
              value: "rabbit@$(MY_POD_NAME).$(K8S_SERVICE_NAME).$(MY_POD_NAMESPACE).svc.cluster.local"
            - name: K8S_HOSTNAME_SUFFIX
              value: ".$(K8S_SERVICE_NAME).$(MY_POD_NAMESPACE).svc.cluster.local"
            - name: RABBITMQ_MNESIA_DIR
              value: "/bitnami/rabbitmq/mnesia/$(RABBITMQ_NODE_NAME)"
            - name: RABBITMQ_LDAP_ENABLE
              value: "no"
            - name: RABBITMQ_LOGS
              value: "-"
            - name: RABBITMQ_ULIMIT_NOFILES
              value: "65536"
            - name: RABBITMQ_USE_LONGNAME
              value: "true"
            - name: RABBITMQ_ERL_COOKIE
              valueFrom:
                secretKeyRef:
                  name: stackstorm-rabbitmq
                  key: rabbitmq-erlang-cookie
            - name: RABBITMQ_USERNAME
              value: "admin"
            - name: RABBITMQ_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: stackstorm-rabbitmq
                  key: rabbitmq-password
            - name: RABBITMQ_PLUGINS
              value: "rabbitmq_management, rabbitmq_peer_discovery_k8s, rabbitmq_auth_backend_ldap"
          ports:
            - name: amqp
              containerPort: 5672
            - name: dist
              containerPort: 25672
            - name: stats
              containerPort: 15672
            - name: epmd
              containerPort: 4369
          livenessProbe:
            exec:
              command:
                - /bin/bash
                - -ec
                - rabbitmq-diagnostics -q check_running
            initialDelaySeconds: 120
            periodSeconds: 30
            timeoutSeconds: 20
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            exec:
              command:
                - /bin/bash
                - -ec
                - rabbitmq-diagnostics -q check_running
            initialDelaySeconds: 10
            periodSeconds: 30
            timeoutSeconds: 20
            successThreshold: 1
            failureThreshold: 3
          resources:
            limits: {}
            requests: {}
          lifecycle:
            preStop:
              exec:
                command:
                  - bash
                  - -ec
                  - rabbitmqctl stop_app
          volumeMounts:
            - name: configuration
              mountPath: /bitnami/rabbitmq/conf
            - name: data
              mountPath: /bitnami/rabbitmq/mnesia
            - name: load-definition-volume
              mountPath: /app
              readOnly: true
      volumes:
        - name: configuration
          configMap:
            name: stackstorm-rabbitmq-config
            items:
              - key: rabbitmq.conf
                path: rabbitmq.conf
        - name: load-definition-volume
          secret:
            secretName: "stackstorm-rabbitmq-definitions"
  volumeClaimTemplates:
    - metadata:
        name: data
        labels:
          app.kubernetes.io/name: rabbitmq
          app.kubernetes.io/instance: stackstorm
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: stackstorm-ha/charts/redis/templates/redis-node-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: stackstorm-redis-node
  namespace: "stackstorm"
  labels:
    app: redis
    chart: redis-12.3.2
    release: stackstorm
    heritage: Helm
spec:
  replicas: 3
  serviceName: stackstorm-redis-headless
  selector:
    matchLabels:
      app: redis
      release: stackstorm
      role: node
  template:
    metadata:
      labels:
        app: redis
        release: stackstorm
        chart: redis-12.3.2
        role: node
      annotations:
        checksum/health: 02faf9bdeab824c62e78e5d01ff1f7f52fafe6263369a28392475d4cd3b80a81
        checksum/configmap: c003883206d97ae44c44ac4e32ec939437fbe797cd083899d2fc6c977eb673ba
        checksum/secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
    spec:
      
      securityContext:
        fsGroup: 1001
      serviceAccountName: default
      containers:
        - name: redis
          image: docker.io/bitnami/redis:6.0.9-debian-10-r66
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          command:
            - /bin/bash
            - -c
            - /opt/bitnami/scripts/start-scripts/start-node.sh
          env:
            - name: REDIS_MASTER_PORT_NUMBER
              value: "6379"
            - name: ALLOW_EMPTY_PASSWORD
              value: "yes"
            - name: REDIS_TLS_ENABLED
              value: "no"
            - name: REDIS_PORT
              value: "6379"
            - name: REDIS_DATA_DIR
              value: /data
          ports:
            - name: redis
              containerPort: 6379
          livenessProbe:
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local.sh 5
          readinessProbe:
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 10
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_readiness_local.sh 5
          resources:
            null
          volumeMounts:
            - name: start-scripts
              mountPath: /opt/bitnami/scripts/start-scripts
            - name: health
              mountPath: /health
            - name: redis-data
              mountPath: /data
            - name: config
              mountPath: /opt/bitnami/redis/mounted-etc
            - name: redis-tmp-conf
              mountPath: /opt/bitnami/redis/etc
        - name: sentinel
          image: docker.io/bitnami/redis-sentinel:6.0.9-debian-10-r66
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          command:
            - /bin/bash
            - -c
            - /opt/bitnami/scripts/start-scripts/start-sentinel.sh
          env:
            - name: ALLOW_EMPTY_PASSWORD
              value: "yes"
            - name: REDIS_SENTINEL_TLS_ENABLED
              value: "no"
            - name: REDIS_SENTINEL_PORT
              value: "26379"
          ports:
            - name: redis-sentinel
              containerPort: 26379
          livenessProbe:
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_sentinel.sh 5
          readinessProbe:
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 1
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_sentinel.sh 5
          resources:
            null
          volumeMounts:
            - name: start-scripts
              mountPath: /opt/bitnami/scripts/start-scripts
            - name: health
              mountPath: /health
            - name: redis-data
              mountPath: /data
              subPath: 
            - name: config
              mountPath: /opt/bitnami/redis-sentinel/mounted-etc
            - name: sentinel-tmp-conf
              mountPath: /opt/bitnami/redis-sentinel/etc
      volumes:
        - name: start-scripts
          configMap:
            name: stackstorm-redis-scripts
            defaultMode: 0755
        - name: health
          configMap:
            name: stackstorm-redis-health
            defaultMode: 0755
        - name: config
          configMap:
            name: stackstorm-redis
        - name: sentinel-tmp-conf
          emptyDir: {}
        - name: redis-tmp-conf
          emptyDir: {}
  volumeClaimTemplates:
    - metadata:
        name: redis-data
        labels:
          app: redis
          release: stackstorm
          heritage: Helm
          component: slave
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
        
        selector:
  updateStrategy:
    type: RollingUpdate
---
# Source: stackstorm-ha/templates/deployments.yaml
# Notify users about breaking change regarding packs, to not destroy current installations
---
# Source: stackstorm-ha/templates/secrets_rabbitmq.yaml
# This configuration is a workaround to https://github.com/bitnami/charts/issues/4635
# This code block should be dropped once the above issue is resolved and definitions can be defined as shown in 
# https://github.com/bitnami/charts/tree/master/bitnami/rabbitmq#load-definitions
---
# Source: stackstorm-ha/templates/tests/st2tests-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "stackstorm-st2tests"
  labels:
    app: st2tests
    tier: tests
    vendor: stackstorm
    chart: stackstorm-ha-0.60.0
    release: stackstorm
    heritage: Helm
  annotations:
    "helm.sh/hook": test-success
    "helm.sh/hook-delete-policy": hook-succeeded
spec:
  initContainers:
  # Sidecar container to copy BATS framework to the main container
  - name: test-framework
    image: dduportal/bats:latest
    command:
      - 'bash'
      - '-ec'
      - |
        cp -R /opt/bats/ ${BATS_HELPERS_DIR} /tools/
    volumeMounts:
      - name: tools
        mountPath: /tools
  # Run the actual BATS tests
  containers:
  - name: st2tests
    image: "stackstorm/st2actionrunner:3.5dev"
    imagePullPolicy: IfNotPresent
    envFrom:
    - configMapRef:
        name: stackstorm-st2-urls
    env:
    - name: BATS_HELPERS_DIR
      value: /tools/bats-helpers/
    - name: ST2_AUTH_USERNAME
      valueFrom:
        secretKeyRef:
          name: stackstorm-st2-auth
          key: username
    - name: ST2_AUTH_PASSWORD
      valueFrom:
        secretKeyRef:
          name: stackstorm-st2-auth
          key: password
    - name: ST2_VERSION
      value: "3.5dev"
    - name: ST2_RBAC_ENABLED
      value: "false"
    volumeMounts:
    - name: tools
      mountPath: /tools
    - name: tests
      mountPath: /tests
    command:
      - /tools/bats/libexec/bats
      - /tests/st2tests.sh
  volumes:
    - name: tools
      emptyDir: {}
    - name: tests
      configMap:
        name: stackstorm-st2tests
  restartPolicy: Never
---
# Source: stackstorm-ha/templates/jobs.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: stackstorm-job-st2-apikey-load
  labels:
    app: st2
    tier: backend
    vendor: stackstorm
    chart: stackstorm-ha-0.60.0
    release: stackstorm
    heritage: Helm
  annotations:
    helm.sh/hook: post-install, post-upgrade, post-rollback
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "6"
spec:
  template:
    metadata:
      name: job-st2-apikey-load
      labels:
        app: st2
        tier: backend
        vendor: stackstorm
        chart: stackstorm-ha-0.60.0
        release: stackstorm
        heritage: Helm
      annotations:
        # TODO: Investigate/propose running Helm hook only on condition when ConfigMap or Secret has changed
        checksum/urls: 03b18c4239e1df1556bbae2fa6ed6e2e37dd493bcce28682c5195640e86a67aa
        checksum/apikeys: 8455e8c50881009a7faeeb47ef3619c9e977e7cd73d0ed17173c72c2572c4a6f
    spec:
      imagePullSecrets:
      initContainers:
            
      - name: wait-for-db
        image: busybox:1.28
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 stackstorm-mongodb-headless 27017 && echo mongodb ok;
              do 
                echo 'Waiting for MongoDB Connection...'
                sleep 2;
            done
      - name: wait-for-api
        image: busybox:1.28
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 stackstorm-st2api 9101 && echo st2api ready;
              do sleep 2;
            done
      # Sidecar container for generating st2client config with st2 username & password pair and sharing produced file with the main container
      - name: generate-st2client-config
        image: "stackstorm/st2actionrunner:3.5dev"
        imagePullPolicy: IfNotPresent
        envFrom:
        - configMapRef:
            name: stackstorm-st2-urls
        env:
        - name: ST2_AUTH_USERNAME
          valueFrom:
            secretKeyRef:
              name: stackstorm-st2-auth
              key: username
        - name: ST2_AUTH_PASSWORD
          valueFrom:
            secretKeyRef:
              name: stackstorm-st2-auth
              key: password
        volumeMounts:
        - name: st2client-config-vol
          mountPath: /root/.st2/
        # `st2 login` doesn't exit on failure correctly, use old methods instead. See bug: https://github.com/StackStorm/st2/issues/4338
        command:
          - 'sh'
          - '-ec'
          - |
            cat <<EOT > /root/.st2/config
            [credentials]
            username = ${ST2_AUTH_USERNAME}
            password = ${ST2_AUTH_PASSWORD}
            EOT
      containers:
      - name: st2-apikey-load
        image: "stackstorm/st2actionrunner:3.5dev"
        imagePullPolicy: IfNotPresent
        command:
          - st2
          - apikey
          - load
          - /etc/st2/apikeys.yaml
        envFrom:
        - configMapRef:
            name: stackstorm-st2-urls
        volumeMounts:
        - name: st2client-config-vol
          mountPath: /root/.st2/
        - name: st2-apikeys-vol
          mountPath: /etc/st2/apikeys.yaml
          subPath: apikeys.yaml
        # TODO: Find out default resource limits for this specific service (#5)
        #resources:
      volumes:
        - name: st2client-config-vol
          emptyDir:
            medium: Memory
        - name: st2-apikeys-vol
          secret:
            secretName: stackstorm-st2-apikeys
      restartPolicy: OnFailure
---
# Source: stackstorm-ha/templates/jobs.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: stackstorm-job-st2-key-load
  labels:
    app: st2
    tier: backend
    vendor: stackstorm
    chart: stackstorm-ha-0.60.0
    release: stackstorm
    heritage: Helm
  annotations:
    helm.sh/hook: post-install, post-upgrade, post-rollback
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "6"
spec:
  template:
    metadata:
      name: job-st2-key-load
      labels:
        app: st2
        tier: backend
        vendor: stackstorm
        chart: stackstorm-ha-0.60.0
        release: stackstorm
        heritage: Helm
      annotations:
        # TODO: Investigate/propose running Helm hook only on condition when ConfigMap or Secret has changed
        checksum/config: 2cecfe3bc433d799634db9c906275d6ffa3ff5eb673367d9046a0acd70b4a77d
        checksum/urls: 03b18c4239e1df1556bbae2fa6ed6e2e37dd493bcce28682c5195640e86a67aa
    spec:
      imagePullSecrets:
      initContainers:
            
      - name: wait-for-db
        image: busybox:1.28
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 stackstorm-mongodb-headless 27017 && echo mongodb ok;
              do 
                echo 'Waiting for MongoDB Connection...'
                sleep 2;
            done
      # Sidecar container for generating st2client config with st2 username & password pair and sharing produced file with the main container
      - name: generate-st2client-config
        image: "stackstorm/st2actionrunner:3.5dev"
        imagePullPolicy: IfNotPresent
        envFrom:
        - configMapRef:
            name: stackstorm-st2-urls
        env:
        - name: ST2_AUTH_USERNAME
          valueFrom:
            secretKeyRef:
              name: stackstorm-st2-auth
              key: username
        - name: ST2_AUTH_PASSWORD
          valueFrom:
            secretKeyRef:
              name: stackstorm-st2-auth
              key: password
        volumeMounts:
        - name: st2client-config-vol
          mountPath: /root/.st2/
        # `st2 login` doesn't exit on failure correctly, use old methods instead. See bug: https://github.com/StackStorm/st2/issues/4338
        command:
          - 'sh'
          - '-ec'
          - |
            cat <<EOT > /root/.st2/config
            [credentials]
            username = ${ST2_AUTH_USERNAME}
            password = ${ST2_AUTH_PASSWORD}
            EOT
      containers:
      - name: st2-key-load
        image: "stackstorm/st2actionrunner:3.5dev"
        imagePullPolicy: IfNotPresent
        command:
          - st2
          - key
          - load
          - /etc/st2/st2kv.yaml
        envFrom:
        - configMapRef:
            name: stackstorm-st2-urls
        volumeMounts:
        - name: st2-config-vol
          mountPath: /etc/st2/st2.docker.conf
          subPath: st2.docker.conf
        - name: st2-config-vol
          mountPath: /etc/st2/st2.user.conf
          subPath: st2.user.conf
        - name: st2client-config-vol
          mountPath: /root/.st2/
        - name: st2-kv-vol
          mountPath: /etc/st2/st2kv.yaml
          subPath: st2kv.yaml
        # TODO: Find out default resource limits for this specific service (#5)
        #resources:
      volumes:
        - name: st2-config-vol
          configMap:
            name: stackstorm-st2-config
        - name: st2client-config-vol
          emptyDir:
            medium: Memory
        - name: st2-kv-vol
          secret:
            secretName: stackstorm-st2-kv
      restartPolicy: OnFailure
---
# Source: stackstorm-ha/templates/jobs.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: stackstorm-job-st2-register-content
  labels:
    app: st2-register-content
    tier: backend
    vendor: stackstorm
    chart: stackstorm-ha-0.60.0
    release: stackstorm
    heritage: Helm
  annotations:
    helm.sh/hook: post-install, post-upgrade, post-rollback
    helm.sh/hook-delete-policy: before-hook-creation
    helm.sh/hook-weight: "7"
spec:
  template:
    metadata:
      name: job-st2-register-content
      labels:
        app: st2-register-content
        tier: backend
        vendor: stackstorm
        chart: stackstorm-ha-0.60.0
        release: stackstorm
        heritage: Helm
      annotations:
        # TODO: Investigate/propose running Helm hook only on condition when ConfigMap or Secret has changed
        checksum/config: 2cecfe3bc433d799634db9c906275d6ffa3ff5eb673367d9046a0acd70b4a77d
        checksum/packs: f7ce2c2f8802015dab601c8f20e98b1657d48c26311bd956ac7b9e0250077051
    spec:
      imagePullSecrets:
      initContainers:
            
      - name: wait-for-db
        image: busybox:1.28
        command:
          - 'sh'
          - '-c'
          - >
            until nc -z -w 2 stackstorm-mongodb-headless 27017 && echo mongodb ok;
              do 
                echo 'Waiting for MongoDB Connection...'
                sleep 2;
            done
      containers:
      - name: st2-register-content
        image: "stackstorm/st2actionrunner:3.5dev"
        imagePullPolicy: IfNotPresent
        command:
          - st2-register-content
          - --config-file=/etc/st2/st2.conf
          - --config-file=/etc/st2/st2.docker.conf
          - --config-file=/etc/st2/st2.user.conf
          - --register-all
          - --register-fail-on-failure
        volumeMounts:
        - name: st2-config-vol
          mountPath: /etc/st2/st2.docker.conf
          subPath: st2.docker.conf
        - name: st2-config-vol
          mountPath: /etc/st2/st2.user.conf
          subPath: st2.user.conf
        - name: st2-pack-configs-vol
          mountPath: /opt/stackstorm/configs/
        # TODO: Find out default resource limits for this specific service (#5)
        #resources:
      volumes:
        - name: st2-config-vol
          configMap:
            name: stackstorm-st2-config
        - name: st2-pack-configs-vol
          configMap:
            name: stackstorm-st2-pack-configs        
      restartPolicy: OnFailure
